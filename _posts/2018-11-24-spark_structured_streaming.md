---
layout: post
title: Spark Structured Streaming
category: diary
tags: []
align: left

---

최근 회사에서 대용량 로그 실시간 분석 시스템을 구축하며 `Spark Structured Streaming`을 사용하고 있다.

<!-- more -->

(작성중...미완성,,,;_;)

나는 서버개발자이긴 하지만 devOps와 비즈니스로직 구현을 위주로 일해왔기에 새로운 시스템을 위한 아키텍처를 설계하고, 사용할 언어 및 프레임워크를 결정하는 일은 제대로 해본적이 없었다. 그리고 이정도로 `대용량` 로그를 다뤄본 적도 없었다. 처음에는 무엇을 어떻게 해야할지 감도 못잡았고.. 여러모로 시행착오도 많았지만 어느정도 완성되어 잘 돌아가고 있는 시스템을 보니 뿌듯함을 이루 다 말할 수 없다. 회사에서 진행한 프로젝트라 소스코드는 올리지 못하겠지만 배우며 진행한 내용을 어느정도 기록해보려고 한다.

먼저 시스템을 구현하며 새로 접한 것들은

- Fluentd
- Kafka (with Zookeeper)
- Hadoop (HDFS, Yarn)
- Spark Structured Streaming
- Scala
- Elasticsearch
- Kibana

모든 것들이 이름 및 용도 정도만 알고 있었지 제대로 다뤄본적이 없었던지라 뭐 하나를 할때마다 쉴새없이 구글링을 해야만 했다. 사실 지금도 잘 안다고는 할 수 없고 여전히 공부중이다. 공부중인데.. 공부할게 너무 많구나 ㅠㅠ

### Requirement
- 3대의 서버에 실시간으로 로그가 쌓이고 있다. 로그는 line by line으로 파일에 append된다.  로그는 Json 포맷이다.
- 각 서버의 로그들을 한군데로 모아서 보관해야 한다.
- 이 로그를 분석하여 분(minute) 단위의 원하는 통계값들을 뽑아내자. AI, 머신러닝 이런것들은 아니고 가공 및 집계 정도의 처리이다.
- 몇분정도의 delay는 허용되나, 가능한 Near Realtime이면 좋겠다.
- 각종 통계값을 시각적으로 확인할 수 있어야 한다.
- 필요에 따라 query를 날려 원하는 통계값을 추출할 수 있어야 한다.
- 통계값을 이용하여 서버에 이상이 없는지 확인이 가능해야 한다.


### Log Details
거래의 트랜잭션과 관련된 로그라서 일반적인 로그와는 다르게 접근할 필요가 있었는데,

- 통계 데이터는 로그가 쓰여진 시간(logged time)이나 실제로 처리되는 시간(processing time) 기준이 아니라 각 로그가 가지고 있는 event time 필드를 기준으로 집계해야 한다. 단 event time과 logged time은 2~3일까지도 차이가 날 수 있다.
- 즉 처음에는 먼저 들어온 로그들에 대해서 통계를 내고, 그 후 나중에 들어오는 로그들까지도 처리를 해서 이전의 통계 결과에 반영을 해줘야 하는 것이다.

사실 지금까지 구현해왔던 통계처리들은 이런 고민을 할 필요가 없었다. 그 이유는

- 1시간단위의 통계면 충분했다.
- 실시간이 아니라, 1시간이 지난 후에 확인해도 충분했다. 즉 00:00~00:59 사이의 통계값을 01:05에 확인해도 충분했다.
- 집계의 기준이 되는 event time과 logeed time이 크게 차이나지 않았다.

따라서 로그 파일을 시간별로 나눠서 저장하고, 매시 5분마다 이전 시간의 파일에 대해서 집계를 하면 충분했다. 한번 생성된 결과는 다시 업데이트되지 않았다. (운영상 문제가 생기는 경우 batch job을 새로 돌리고 기존의 결과를 덮어썼다.)

즉 logged time를 기준으로 집계를 한것이기에 정확한 값은 아니었다. 하지만 각 로그는 단일 데이터라서 다른 로그와의 연관성이 없고, 02:00에 발생된 로그가 1시의 통계에 포함되던 2시의 통계에 포함되던 어느쪽이든 딱 한 번 포함되기만 하면 괜찮았다.

하지만 이번에 처리하는 로그는 이런 방식을 사용할 수가 없었다. 만약 로그가 최대 24시간까지 늦게 들어온다고 하면 아예 통계처리를 24시간이 지난 후 하면 가능하긴 하다. 하지만 원하는 것은 실시간 확인이었다. 또한 반드시 event time을 기준으로 집계해야 의미가 있었다. 따라서 이전에 해오던 batch 처리 방식과는 다른 방식이 필요했다. 

나는 RDB에 친숙하기 때문에, 간단하게는 이런 방법을 생각할 수 있다.
DB에 테이블을 만들어놓고, 부분적으로 생성되는 통계값들의 결과를 + 시켜주는 것이다. `insert ... on duplicate key update column += value` 이런 처리 말이다. 그러면 늦게 들어오는 데이터들도 다 처리는 할 수 있다. 하지만 이렇게 처리하면 여러가지 문제가 있는데.. 어쩌고저쩌고

이런 상황에 필요한 처리를 지원하는 많은 Streaming 프레임워크들이 있었고, 이미 나 빼고 모든 사람들이 사용하고 있는 듯 했다... ㅋㅋㅋㅋ 아무튼 그 중에서도 Spark Structured Streaming를 사용하기로 결정했다. why는 나중에 정리해야지..

### Spark Structured Streaming

* https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html
  * 이 포스팅 외에도 databricks 블로그에 있는 structured streaming 관련 글 모두 설명이 참 잘 되어있다.
* https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html
  * 공식 문서 또한 완전 잘 되어있다. 궁금한 것 대부분은 여기에서 답을 찾을 수 있다.
* https://github.com/databricks/scala-style-guide
  * Scala를 써보는 것도 처음이라.. 코딩 스타일 가이드이다.



시간이 날때마다 이어서 써야지..


